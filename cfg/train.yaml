# Dataset
dataset: data/data_3.json

# Model configuration
model_name: "meta-llama/Llama-2-7b-hf"  # Pretrained model from Hugging Face

# Training parameters
max_length: 1024  # Max sequence length for tokenization
output_dir: "output"  # Directory to save model checkpoints and logs
logging_dir: "logs"  # Directory for logging
learning_rate: 0.005  # Learning rate
batch_size: 8  # Per-device batch size
epochs: 3  # Number of epochs
grad_accumulation_steps: 16  # Gradient accumulation for large effective batch size
save_total_limit: 2  # Keep only the last 2 checkpoints
save_strategy: "epoch"  # Save checkpoint at the end of each epoch
evaluation_strategy: "epoch"  # Evaluate the model at the end of each epoch

# Distributed Training
fp16: true  # Use mixed precision for faster training and lower memory usage
ddp_find_unused_parameters: false  # For DistributedDataParallel optimization

# Other options
logging_steps: 10  # Log metrics every 10 steps
load_best_model_at_end: true  # Load the best checkpoint at the end